# AI4Science

<p align="center">
  <img src="https://img.shields.io/badge/iOS-26%2B-blue" alt="iOS 26+">
  <img src="https://img.shields.io/badge/Swift-6.2-orange" alt="Swift 6.2">
  <img src="https://img.shields.io/badge/SwiftUI-Observation-green" alt="SwiftUI">
  <img src="https://img.shields.io/badge/License-MIT-lightgrey" alt="License">
</p>

A citizen science iOS application developed by **UTSA's Vision & AI Lab** that enables researchers and citizens to capture, label, and analyze materials science data using on-device machine learning.

## Overview

AI4Science bridges the gap between laboratory research and citizen science by providing a mobile platform for materials analysis. Scientists can design studies, while participants capture high-quality images of materials, annotate defects, and contribute to researchâ€”all with real-time AI assistance running entirely on-device.

### Key Capabilities

- **ğŸ“¸ High-Quality Capture** â€” Photo and video capture optimized for materials science with RAW support
- **ğŸ¤– On-Device ML** â€” Defect detection and classification using CoreML, optimized for Neural Engine
- **ğŸ·ï¸ Smart Annotation** â€” AI-assisted labeling with multiple annotation types (points, rectangles, polygons, freeform)
- **ğŸ“Š Real-Time Analysis** â€” Instant feedback on captured samples with confidence scores
- **ğŸ”¬ ResearchKit Integration** â€” Surveys, consent flows, and study management for citizen science
- **ğŸ“¡ Offline-First** â€” Full functionality without internet; sync when connected
- **ğŸ¯ AR Overlays** â€” Real-time defect visualization using ARKit

## Requirements

- **iOS 26.0+**
- **Xcode 26.2+**
- **Swift 6.2**
- iPhone with A12 Bionic or later (for Neural Engine ML acceleration)

## Getting Started

### Clone the Repository

```bash
git clone git@github.com:gabecfernandez/AI4Science.git
cd AI4Science
```

### Open in Xcode

```bash
open AI4Science.xcodeproj
```

### Build & Run

1. Select your target device or simulator
2. Press `Cmd + R` to build and run

### Running Tests

```bash
xcodebuild test \
  -scheme AI4Science \
  -destination 'platform=iOS Simulator,name=iPhone 16'
```

## Architecture

AI4Science follows **MVVM + Repository + Clean Architecture** with strict separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Presentation Layer                        â”‚
â”‚         Views (SwiftUI) â†â†’ ViewModels (@Observable)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Domain Layer                            â”‚
â”‚              Use Cases â€¢ Business Logic                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Data Layer                             â”‚
â”‚         Repositories â€¢ SwiftData â€¢ Mappers â€¢ Sync           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Infrastructure Layer                      â”‚
â”‚        Camera â€¢ ML â€¢ AR â€¢ ResearchKit â€¢ Network             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure

```
AI4Science/
â”œâ”€â”€ Core/                   # Shared models, protocols, utilities
â”‚   â”œâ”€â”€ Models/            # Domain models (User, Project, Sample, etc.)
â”‚   â”œâ”€â”€ Protocols/         # Repository, UseCase, Service protocols
â”‚   â””â”€â”€ Extensions/        # Swift type extensions
â”œâ”€â”€ Data/                   # Data persistence layer
â”‚   â”œâ”€â”€ SwiftData/         # @Model entities
â”‚   â”œâ”€â”€ Repositories/      # Data access implementations
â”‚   â””â”€â”€ Sync/              # Offline sync coordination
â”œâ”€â”€ Domain/                 # Business logic
â”‚   â””â”€â”€ UseCases/          # Feature-specific use cases
â”œâ”€â”€ Features/               # MVVM feature modules
â”‚   â”œâ”€â”€ Auth/              # Authentication
â”‚   â”œâ”€â”€ Projects/          # Project management
â”‚   â”œâ”€â”€ Capture/           # Camera & capture
â”‚   â”œâ”€â”€ Analysis/          # ML analysis results
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Infrastructure/         # Platform services
â”‚   â”œâ”€â”€ Camera/            # AVFoundation integration
â”‚   â”œâ”€â”€ ML/                # CoreML + Vision
â”‚   â”œâ”€â”€ AR/                # ARKit overlays
â”‚   â””â”€â”€ ResearchKit/       # Survey & consent
â””â”€â”€ UI/                     # Reusable components
    â”œâ”€â”€ Components/        # Buttons, Cards, Inputs, etc.
    â””â”€â”€ DesignSystem/      # Colors, Typography, Spacing
```

## Features

### For Researchers

- **Study Design** â€” Create projects with custom protocols and consent flows
- **Sample Management** â€” Organize materials by project, batch, and metadata
- **Data Export** â€” Export annotations and analysis results in JSON, CSV, or ZIP formats
- **NSF FAIR Compliance** â€” Built-in support for open science data standards
- **Lab Affiliation** â€” Connect with UTSA labs and research groups

### For Citizen Scientists

- **Guided Capture** â€” Step-by-step instructions for capturing quality images
- **AI Assistance** â€” Real-time defect detection helps identify areas of interest
- **Progress Tracking** â€” See contribution history and impact
- **Offline Mode** â€” Capture and annotate without internet connection
- **ResearchKit Surveys** â€” Participate in research studies

### Machine Learning

The app includes on-device ML models for:

| Model | Purpose | Input |
|-------|---------|-------|
| DefectDetector | Identify material defects | 640Ã—640 image |
| MaterialClassifier | Classify material types | 224Ã—224 image |
| Segmentation | Pixel-level defect masks | 512Ã—512 image |

All models run locally using CoreML with Neural Engine optimizationâ€”no data leaves the device.

## Swift Concurrency

AI4Science is built with Swift 6.2's strict concurrency from the ground up:

```swift
// ViewModels are @MainActor isolated
@Observable
@MainActor
final class ProjectsViewModel {
    private let repository: ProjectRepository
    var projects: [Project] = []

    func loadProjects() async {
        projects = try await repository.findAll()
    }
}

// Services use actor isolation
actor CameraManager {
    private var session: AVCaptureSession?

    func startSession() async throws {
        // Thread-safe camera operations
    }
}
```

## Configuration

### Environment Variables

Create a `.env` file for local development:

```bash
# API Configuration (optional - app works offline)
API_BASE_URL=https://api.ai4science.utsa.edu
API_VERSION=v1

# Feature Flags
ENABLE_AR_OVERLAY=true
ENABLE_CLOUD_SYNC=false
```

### Build Configurations

| Configuration | Purpose |
|---------------|---------|
| Debug | Development with verbose logging |
| Release | Production build with optimizations |

## Testing

The project uses **Swift Testing** framework:

```swift
@Suite("Project Tests")
struct ProjectTests {

    @Test("Project initializes with correct status")
    func testProjectInit() {
        let project = Project(name: "Test", ownerId: UUID())
        #expect(project.status == .draft)
    }

    @Test("Repository saves and retrieves")
    @MainActor
    func testRepositoryCRUD() async throws {
        let repo = MockProjectRepository()
        let project = Project(name: "Test", ownerId: UUID())

        try await repo.save(project)
        let retrieved = try await repo.findById(project.id)

        #expect(retrieved?.name == "Test")
    }
}
```

### Test Coverage

- **Core/** â€” Model validation, serialization
- **Data/** â€” Repository CRUD operations
- **Domain/** â€” Use case business logic
- **Features/** â€” ViewModel state management
- **Infrastructure/** â€” Service integration

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- Follow Swift API Design Guidelines
- Use `@MainActor` for all ViewModels
- Prefer `async/await` over callbacks
- Write tests for new features

## Documentation

- [ARCHITECTURE.md](AI4Science/ARCHITECTURE.md) â€” Detailed architecture overview
- [CLAUDE.md](CLAUDE.md) â€” AI assistant guidance for the codebase

## Research & Publications

This app supports research at UTSA's Vision & AI Lab. If you use AI4Science in your research, please cite:

```bibtex
@software{ai4science2026,
  title = {AI4Science: Mobile Citizen Science for Materials Analysis},
  author = {Fernandez, Gabriel and UTSA Vision & AI Lab},
  year = {2026},
  url = {https://github.com/gabecfernandez/AI4Science}
}
```

## License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **UTSA Vision & AI Lab** â€” Research direction and domain expertise
- **Apple** â€” SwiftUI, SwiftData, CoreML, ResearchKit frameworks
- **NSF** â€” Support for open science initiatives

